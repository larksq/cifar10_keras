{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "from os.path import isfile, join\n",
    "\n",
    "#from spatial_transformer import SpatialTransformer\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import misc\n",
    "import glob\n",
    "def load_train_HSV(img_path):\n",
    "    imageArr = np.zeros((1, 32, 32, 3))\n",
    "    iCount = 1\n",
    "    print('total:',len(glob.glob(img_path)))\n",
    "    for image_path in glob.glob(img_path):#(\"test/*.png\"):\n",
    "        img = cv2.imread( image_path )\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        #convert to gray\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        img = img.reshape(1,32,32,3)\n",
    "        imageArr = np.concatenate((imageArr, img), axis=0)\n",
    "        # print (image.shape)\n",
    "        # print (image.dtype)\n",
    "        #(32, 32, 3)   uint8\n",
    "        if iCount%500 == 0:\n",
    "            print (\"500new loaded:\",imageArr.shape)\n",
    "        iCount += 1\n",
    "        # print (imageArr.shape)\n",
    "    print(imageArr.shape)\n",
    "    imageArr = np.delete(imageArr, 0, axis=0)#imageArr[0], axis=0)\n",
    "    print (\"images imported: \",imageArr.shape)\n",
    "    return imageArr\n",
    "\n",
    "def load_train(img_path):\n",
    "    imageArr = np.zeros((1, 32, 32, 3))\n",
    "    iCount = 1\n",
    "    print('total:',len(glob.glob(img_path)))\n",
    "    for image_path in sorted(glob.glob(img_path), key=lambda name: int(name[6:-4])):#(\"test/*.png\"):\n",
    "        img = cv2.imread( image_path )\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        #convert to rgb\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.reshape(1,32,32,3)\n",
    "        imageArr = np.concatenate((imageArr, img), axis=0)\n",
    "        # print (image.shape)\n",
    "        # print (image.dtype)\n",
    "        #(32, 32, 3)   uint8\n",
    "        if iCount%500 == 0:\n",
    "            print (\"500new loaded:\",imageArr.shape,image_path)\n",
    "        iCount += 1\n",
    "        # print (imageArr.shape)\n",
    "    print(imageArr.shape)\n",
    "    imageArr = np.delete(imageArr, 0, axis=0)#imageArr[0], axis=0)\n",
    "    print (\"images imported: \",imageArr.shape)\n",
    "    return imageArr\n",
    "\n",
    "def load_test(img_path, start, end):\n",
    "    imageArr = np.zeros((1, 32, 32, 3))\n",
    "    iCount = 1\n",
    "    print('total:',len(sorted(glob.glob(img_path), key=lambda name: int(name[5:-4]))))\n",
    "    for image_path in sorted(glob.glob(img_path), key=lambda name: int(name[5:-4]))[start:end]:#(\"test/*.png\"):\n",
    "        img = cv2.imread( image_path )\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        #convert to rgb\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.reshape(1,32,32,3)\n",
    "        imageArr = np.concatenate((imageArr, img), axis=0)\n",
    "        # print (image.shape)\n",
    "        # print (image.dtype)\n",
    "        #(32, 32, 3)   uint8\n",
    "        if iCount%500 == 0:\n",
    "            print (\"500new loaded:\",imageArr.shape,image_path)\n",
    "        iCount += 1\n",
    "        # print (imageArr.shape)\n",
    "    print(imageArr.shape)\n",
    "    imageArr = np.delete(imageArr, 0, axis=0)#imageArr[0], axis=0)\n",
    "    print (\"images imported: \",imageArr.shape)\n",
    "    return imageArr\n",
    "\n",
    "def load_labels(file_path):\n",
    "    mnist_df = pd.read_csv(file_path, index_col=None)\n",
    "    print('train label sizes:', mnist_df.shape)\n",
    "    #labels = mnist_df.iloc[0:mnist_df.shape[0],0:1]\n",
    "    labels = mnist_df.iloc[0:mnist_df.shape[0],1:2]\n",
    "    labels_data = labels.values\n",
    "    print('labels:',labels.shape, labels.shape[-1])\n",
    "    print('labels sample:',labels_data[:10])\n",
    "    return labels_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "\n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_path = os.path.join('cache', 'train.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_train = load_train('train/*.png')\n",
    "    y_train = load_labels('trainLabels.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(\"cache data:\",X_train.shape, y_train.shape, \"y sample:\", y_train[:10])\n",
    "    print(y_train)\n",
    "    cache_data((X_train, y_train), cache_path)\n",
    "    print(\"cache_data\")\n",
    "else:\n",
    "    print('Restore train from cache!')\n",
    "    (X_train, y_train) = restore_data(cache_path)\n",
    "    print(X_train.shape, y_train.shape, \"y sample:\", y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2])        \n",
    "for i in range(0, 4):\n",
    "    plt.subplot(330 + (i+1))\n",
    "    plt.imshow(X_train[i])\n",
    "    plt.title(y_train[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "testa = [0,1,2]\n",
    "testb = [3]\n",
    "testc = [4]\n",
    "\n",
    "testa = np.concatenate((testa, testb), axis=0)\n",
    "testa = np.concatenate((testa, testc), axis=0)\n",
    "\n",
    "print(testa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 50000\n",
      "500new loaded: (501, 32, 32, 3)\n",
      "500new loaded: (1001, 32, 32, 3)\n",
      "500new loaded: (1501, 32, 32, 3)\n",
      "500new loaded: (2001, 32, 32, 3)\n",
      "500new loaded: (2501, 32, 32, 3)\n",
      "500new loaded: (3001, 32, 32, 3)\n",
      "500new loaded: (3501, 32, 32, 3)\n",
      "500new loaded: (4001, 32, 32, 3)\n",
      "500new loaded: (4501, 32, 32, 3)\n",
      "500new loaded: (5001, 32, 32, 3)\n",
      "500new loaded: (5501, 32, 32, 3)\n",
      "500new loaded: (6001, 32, 32, 3)\n",
      "500new loaded: (6501, 32, 32, 3)\n",
      "500new loaded: (7001, 32, 32, 3)\n",
      "500new loaded: (7501, 32, 32, 3)\n",
      "500new loaded: (8001, 32, 32, 3)\n",
      "500new loaded: (8501, 32, 32, 3)\n",
      "500new loaded: (9001, 32, 32, 3)\n",
      "500new loaded: (9501, 32, 32, 3)\n",
      "500new loaded: (10001, 32, 32, 3)\n",
      "500new loaded: (10501, 32, 32, 3)\n",
      "500new loaded: (11001, 32, 32, 3)\n",
      "500new loaded: (11501, 32, 32, 3)\n",
      "500new loaded: (12001, 32, 32, 3)\n",
      "500new loaded: (12501, 32, 32, 3)\n",
      "500new loaded: (13001, 32, 32, 3)\n",
      "500new loaded: (13501, 32, 32, 3)\n",
      "500new loaded: (14001, 32, 32, 3)\n",
      "500new loaded: (14501, 32, 32, 3)\n",
      "500new loaded: (15001, 32, 32, 3)\n",
      "500new loaded: (15501, 32, 32, 3)\n",
      "500new loaded: (16001, 32, 32, 3)\n",
      "500new loaded: (16501, 32, 32, 3)\n",
      "500new loaded: (17001, 32, 32, 3)\n",
      "500new loaded: (17501, 32, 32, 3)\n",
      "500new loaded: (18001, 32, 32, 3)\n",
      "500new loaded: (18501, 32, 32, 3)\n",
      "500new loaded: (19001, 32, 32, 3)\n",
      "500new loaded: (19501, 32, 32, 3)\n",
      "500new loaded: (20001, 32, 32, 3)\n",
      "500new loaded: (20501, 32, 32, 3)\n",
      "500new loaded: (21001, 32, 32, 3)\n",
      "500new loaded: (21501, 32, 32, 3)\n",
      "500new loaded: (22001, 32, 32, 3)\n",
      "500new loaded: (22501, 32, 32, 3)\n",
      "500new loaded: (23001, 32, 32, 3)\n",
      "500new loaded: (23501, 32, 32, 3)\n",
      "500new loaded: (24001, 32, 32, 3)\n",
      "500new loaded: (24501, 32, 32, 3)\n",
      "500new loaded: (25001, 32, 32, 3)\n",
      "500new loaded: (25501, 32, 32, 3)\n",
      "500new loaded: (26001, 32, 32, 3)\n",
      "500new loaded: (26501, 32, 32, 3)\n",
      "500new loaded: (27001, 32, 32, 3)\n",
      "500new loaded: (27501, 32, 32, 3)\n",
      "500new loaded: (28001, 32, 32, 3)\n",
      "500new loaded: (28501, 32, 32, 3)\n",
      "500new loaded: (29001, 32, 32, 3)\n",
      "500new loaded: (29501, 32, 32, 3)\n",
      "500new loaded: (30001, 32, 32, 3)\n",
      "500new loaded: (30501, 32, 32, 3)\n",
      "500new loaded: (31001, 32, 32, 3)\n",
      "500new loaded: (31501, 32, 32, 3)\n",
      "500new loaded: (32001, 32, 32, 3)\n",
      "500new loaded: (32501, 32, 32, 3)\n",
      "500new loaded: (33001, 32, 32, 3)\n",
      "500new loaded: (33501, 32, 32, 3)\n",
      "500new loaded: (34001, 32, 32, 3)\n",
      "500new loaded: (34501, 32, 32, 3)\n",
      "500new loaded: (35001, 32, 32, 3)\n",
      "500new loaded: (35501, 32, 32, 3)\n",
      "500new loaded: (36001, 32, 32, 3)\n",
      "500new loaded: (36501, 32, 32, 3)\n",
      "500new loaded: (37001, 32, 32, 3)\n",
      "500new loaded: (37501, 32, 32, 3)\n",
      "500new loaded: (38001, 32, 32, 3)\n",
      "500new loaded: (38501, 32, 32, 3)\n",
      "500new loaded: (39001, 32, 32, 3)\n",
      "500new loaded: (39501, 32, 32, 3)\n",
      "500new loaded: (40001, 32, 32, 3)\n",
      "500new loaded: (40501, 32, 32, 3)\n",
      "500new loaded: (41001, 32, 32, 3)\n",
      "500new loaded: (41501, 32, 32, 3)\n",
      "500new loaded: (42001, 32, 32, 3)\n",
      "500new loaded: (42501, 32, 32, 3)\n",
      "500new loaded: (43001, 32, 32, 3)\n",
      "500new loaded: (43501, 32, 32, 3)\n",
      "500new loaded: (44001, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'trainHSV.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_train = load_train_HSV('train/*.png')\n",
    "    y_train = load_labels('trainLabels.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(\"cache data:\",X_train.shape, y_train.shape, \"y sample:\", y_train[:10])\n",
    "    print(y_train)\n",
    "    cache_data((X_train, y_train), cache_path)\n",
    "    print(\"cache_data\")\n",
    "else:\n",
    "    print('Restore train from cache!')\n",
    "    (X_train, y_train) = restore_data(cache_path)\n",
    "    print(X_train.shape, y_train.shape, \"y sample:\", y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 300000\n",
      "(11, 32, 32, 3)\n",
      "images imported:  (10, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAC8CAYAAAAHMstWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0VXQF59CoVAougr/P5B9yjy+19aKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f623438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2]) \n",
    "X_test = load_test('test/*.png', 0, 10)\n",
    "for i in range(0, 4):\n",
    "    plt.subplot(330 + (i+1))\n",
    "    plt.imshow(X_test[i])\n",
    "    plt.title(y_train[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test0-50.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 0, 50000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(30000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test50-80.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 50000, 80000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(30000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test80-110.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 80000, 110000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(30000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test110-140.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 110000, 140000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(30000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test140-170.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 140000, 170000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(30000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test170-200.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 170000, 200000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(30000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test200-230.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 200000, 230000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test250-300.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 250000, 300000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore test from cache!\n",
      "(20000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cache_path = os.path.join('cache', 'test230-250.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    # train_data, train_target = load_train()\n",
    "    X_test = load_test('test/*.png', 230000, 250000)#load_train('test/*.png')\n",
    "    #y_test = load_labels('train.csv')\n",
    "    #X_train, y_train = load_train_dic('onlyletter/')\n",
    "    # load_train('trimeddataset/g1')\n",
    "    print(X_test.shape)\n",
    "    cache_data(X_test, cache_path)\n",
    "    print(\"cache_data for test\")\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    X_test = restore_data(cache_path)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (50000, 10) y sample: [[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#save the uncoded labels\n",
    "labels_data = y_train\n",
    "# encode class values as integers\n",
    "y_train = np.ravel(y_train)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# print(encoded_Y.shape, encoded_Y) \n",
    "# [0,1,2,3...,34,35]\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# print(dummy_y.shape, dummy_y)\n",
    "# [1,0,0....0][0,1,0....,0][0,0,1,...,0]\n",
    "\n",
    "y_train = dummy_y\n",
    "print(\"y shape:\",y_train.shape,\"y sample:\",y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (32,32,3)\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 400\n",
    "data_augmentation = True\n",
    "\n",
    "# for i in range(5, 9):\n",
    "#     plt.subplot(330 + (i+1))\n",
    "#     plt.imshow(imgTrain[i], cmap=plt.get_cmap('gray'))\n",
    "#     plt.title(labels_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,CSVLogger,ModelCheckpoint, TensorBoard\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "# optimizers = ['adam','rmsprop']\n",
    "# activations = ['relu','PReLU','LeakyReLU']\n",
    "# activations_ps = ['relu']\n",
    "# dropouts = [0.6,0.5,0,0.3]\n",
    "# # padding = ['same','valid']\n",
    "# cost_fun = [loss_max,'mse']\n",
    "\n",
    "# checkpoint (only weights here)\n",
    "filepath=\"backups/weights.best.hdf5\"#\"backups/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',patience=2,verbose=1)\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "tensorboard = TensorBoard(\"logs/run_a\")\n",
    "\n",
    "callbacks = [earlystopping, checkpoint, tensorboard]\n",
    "\n",
    "# use following code before model.compile to load saved weights\n",
    "# load weights\n",
    "# model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "# run this in a seperate terminial \n",
    "# tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_trainK, y_trainK), (x_testK, y_testK) = cifar10.load_data()\n",
    "# x_trainK = x_trainK.reshape(x_trainK.shape[0], x_trainK.shape[1], x_trainK.shape[2], 3)\n",
    "# x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\n",
    "# y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "# y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "y_trainK = keras.utils.to_categorical(y_trainK, num_classes)\n",
    "y_testK = keras.utils.to_categorical(y_testK, num_classes)\n",
    "print (x_trainK.shape, y_trainK.shape, x_testK.shape, y_testK.shape)\n",
    "print (\"y samples:\", y_testK[:10])\n",
    "x_trainK = np.concatenate((x_testK, x_trainK), axis=0)\n",
    "y_trainK = np.concatenate((y_testK, y_trainK), axis=0)\n",
    "# X_train = np.concatenate((x_trainK, X_train), axis=0)\n",
    "# X_train = np.concatenate((x_testK, X_train), axis=0)\n",
    "# y_train = np.concatenate((y_trainK, y_train), axis=0)\n",
    "# y_train = np.concatenate((y_testK, y_train), axis=0)\n",
    "\n",
    "#np.random.shuffle(images_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"k samples:\",x_trainK[:5])\n",
    "print(\"train samples:\",X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "\n",
    "def selu(x):\n",
    "    import tensorflow as tf\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale*tf.where(x >= 0.0, x, alpha*tf.nn.elu(x))\n",
    "\n",
    "# initial weights\n",
    "# b = np.zeros((2, 3), dtype='float32')\n",
    "# b[0, 0] = 1\n",
    "# b[1, 1] = 1\n",
    "# W = np.zeros((50, 6), dtype='float32')\n",
    "# weights = [W, b.flatten()]\n",
    "\n",
    "# locnet = Sequential()\n",
    "# locnet.add(MaxPooling2D(pool_size=(2,2), input_shape=input_shape))\n",
    "# locnet.add(Conv2D(20, (5, 5)))\n",
    "# locnet.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# locnet.add(Conv2D(20, (5, 5)))\n",
    "\n",
    "# locnet.add(Flatten())\n",
    "# locnet.add(Dense(50))\n",
    "# locnet.add(Activation('relu'))\n",
    "# locnet.add(Dense(6, weights=weights))\n",
    "#locnet.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_input(x):\n",
    "    mean = X_train.mean().astype(np.float32)\n",
    "    std = X_train.std().astype(np.float32)\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_deeper_cnn_model():\n",
    "    model = Sequential()\n",
    "    #model.add(SpatialTransformer(localization_net=locnet, output_size=(30, 30), input_shape=input_shape))\n",
    "    #model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization()) # customized \n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "#     #customized op\n",
    "#     from keras import optimizers\n",
    "#     ad10e4 = optimizers.Adam(lr=0.0005)\n",
    "#     #commpile modelPython\n",
    "#     model.compile(optimizer=ad10e4,\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample: [[[[ 0.23137255  0.24313726  0.24705882]\n",
      "   [ 0.16862746  0.18039216  0.17647059]\n",
      "   [ 0.19607843  0.1882353   0.16862746]\n",
      "   ..., \n",
      "   [ 0.61960787  0.51764709  0.42352942]\n",
      "   [ 0.59607846  0.49019608  0.40000001]\n",
      "   [ 0.58039218  0.48627451  0.40392157]]\n",
      "\n",
      "  [[ 0.0627451   0.07843138  0.07843138]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.07058824  0.03137255  0.        ]\n",
      "   ..., \n",
      "   [ 0.48235294  0.34509805  0.21568628]\n",
      "   [ 0.46666667  0.32549021  0.19607843]\n",
      "   [ 0.47843137  0.34117648  0.22352941]]\n",
      "\n",
      "  [[ 0.09803922  0.09411765  0.08235294]\n",
      "   [ 0.0627451   0.02745098  0.        ]\n",
      "   [ 0.19215687  0.10588235  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4627451   0.32941177  0.19607843]\n",
      "   [ 0.47058824  0.32941177  0.19607843]\n",
      "   [ 0.42745098  0.28627452  0.16470589]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81568629  0.66666669  0.3764706 ]\n",
      "   [ 0.78823531  0.60000002  0.13333334]\n",
      "   [ 0.7764706   0.63137257  0.10196079]\n",
      "   ..., \n",
      "   [ 0.627451    0.52156866  0.27450982]\n",
      "   [ 0.21960784  0.12156863  0.02745098]\n",
      "   [ 0.20784314  0.13333334  0.07843138]]\n",
      "\n",
      "  [[ 0.70588237  0.54509807  0.3764706 ]\n",
      "   [ 0.67843139  0.48235294  0.16470589]\n",
      "   [ 0.72941178  0.56470591  0.11764706]\n",
      "   ..., \n",
      "   [ 0.72156864  0.58039218  0.36862746]\n",
      "   [ 0.38039216  0.24313726  0.13333334]\n",
      "   [ 0.32549021  0.20784314  0.13333334]]\n",
      "\n",
      "  [[ 0.69411767  0.56470591  0.45490196]\n",
      "   [ 0.65882355  0.50588238  0.36862746]\n",
      "   [ 0.7019608   0.55686277  0.34117648]\n",
      "   ..., \n",
      "   [ 0.84705883  0.72156864  0.54901963]\n",
      "   [ 0.59215689  0.4627451   0.32941177]\n",
      "   [ 0.48235294  0.36078432  0.28235295]]]\n",
      "\n",
      "\n",
      " [[[ 0.60392159  0.69411767  0.73333335]\n",
      "   [ 0.49411765  0.53725493  0.53333336]\n",
      "   [ 0.41176471  0.40784314  0.37254903]\n",
      "   ..., \n",
      "   [ 0.35686275  0.37254903  0.27843139]\n",
      "   [ 0.34117648  0.35294119  0.27843139]\n",
      "   [ 0.30980393  0.31764707  0.27450982]]\n",
      "\n",
      "  [[ 0.54901963  0.627451    0.66274512]\n",
      "   [ 0.56862748  0.60000002  0.60392159]\n",
      "   [ 0.49019608  0.49019608  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.3764706   0.3882353   0.30588236]\n",
      "   [ 0.3019608   0.3137255   0.24313726]\n",
      "   [ 0.27843139  0.28627452  0.23921569]]\n",
      "\n",
      "  [[ 0.54901963  0.60784316  0.64313728]\n",
      "   [ 0.54509807  0.57254905  0.58431375]\n",
      "   [ 0.4509804   0.4509804   0.43921569]\n",
      "   ..., \n",
      "   [ 0.30980393  0.32156864  0.25098041]\n",
      "   [ 0.26666668  0.27450982  0.21568628]\n",
      "   [ 0.26274511  0.27058825  0.21568628]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.68627453  0.65490198  0.65098041]\n",
      "   [ 0.61176473  0.60392159  0.627451  ]\n",
      "   [ 0.60392159  0.627451    0.66666669]\n",
      "   ..., \n",
      "   [ 0.16470589  0.13333334  0.14117648]\n",
      "   [ 0.23921569  0.20784314  0.22352941]\n",
      "   [ 0.36470589  0.32549021  0.35686275]]\n",
      "\n",
      "  [[ 0.64705884  0.60392159  0.50196081]\n",
      "   [ 0.61176473  0.59607846  0.50980395]\n",
      "   [ 0.62352943  0.63137257  0.55686277]\n",
      "   ..., \n",
      "   [ 0.40392157  0.36470589  0.3764706 ]\n",
      "   [ 0.48235294  0.44705883  0.47058824]\n",
      "   [ 0.51372552  0.47450981  0.51372552]]\n",
      "\n",
      "  [[ 0.63921571  0.58039218  0.47058824]\n",
      "   [ 0.61960787  0.58039218  0.47843137]\n",
      "   [ 0.63921571  0.61176473  0.52156866]\n",
      "   ..., \n",
      "   [ 0.56078434  0.52156866  0.54509807]\n",
      "   [ 0.56078434  0.52549022  0.55686277]\n",
      "   [ 0.56078434  0.52156866  0.56470591]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   ..., \n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.44313726  0.47058824  0.43921569]\n",
      "   [ 0.43529412  0.4627451   0.43529412]\n",
      "   [ 0.41176471  0.43921569  0.41568628]\n",
      "   ..., \n",
      "   [ 0.28235295  0.31764707  0.3137255 ]\n",
      "   [ 0.28235295  0.3137255   0.30980393]\n",
      "   [ 0.28235295  0.3137255   0.30980393]]\n",
      "\n",
      "  [[ 0.43529412  0.4627451   0.43137255]\n",
      "   [ 0.40784314  0.43529412  0.40784314]\n",
      "   [ 0.3882353   0.41568628  0.38431373]\n",
      "   ..., \n",
      "   [ 0.26666668  0.29411766  0.28627452]\n",
      "   [ 0.27450982  0.29803923  0.29411766]\n",
      "   [ 0.30588236  0.32941177  0.32156864]]\n",
      "\n",
      "  [[ 0.41568628  0.44313726  0.41176471]\n",
      "   [ 0.3882353   0.41568628  0.38431373]\n",
      "   [ 0.37254903  0.40000001  0.36862746]\n",
      "   ..., \n",
      "   [ 0.30588236  0.33333334  0.32549021]\n",
      "   [ 0.30980393  0.33333334  0.32549021]\n",
      "   [ 0.3137255   0.33725491  0.32941177]]]\n",
      "\n",
      "\n",
      " [[[ 0.10980392  0.09803922  0.03921569]\n",
      "   [ 0.14509805  0.13333334  0.07450981]\n",
      "   [ 0.14901961  0.13725491  0.07843138]\n",
      "   ..., \n",
      "   [ 0.29803923  0.26274511  0.15294118]\n",
      "   [ 0.31764707  0.28235295  0.16862746]\n",
      "   [ 0.33333334  0.29803923  0.18431373]]\n",
      "\n",
      "  [[ 0.12941177  0.10980392  0.05098039]\n",
      "   [ 0.13333334  0.11764706  0.05490196]\n",
      "   [ 0.1254902   0.10588235  0.04705882]\n",
      "   ..., \n",
      "   [ 0.37254903  0.32156864  0.21568628]\n",
      "   [ 0.3764706   0.32156864  0.21960784]\n",
      "   [ 0.33333334  0.28235295  0.17647059]]\n",
      "\n",
      "  [[ 0.15294118  0.1254902   0.05882353]\n",
      "   [ 0.15686275  0.12941177  0.06666667]\n",
      "   [ 0.22352941  0.19607843  0.12941177]\n",
      "   ..., \n",
      "   [ 0.36470589  0.29803923  0.20392157]\n",
      "   [ 0.41960785  0.34901962  0.25882354]\n",
      "   [ 0.37254903  0.3019608   0.21176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.32549021  0.28627452  0.20392157]\n",
      "   [ 0.34117648  0.3019608   0.21960784]\n",
      "   [ 0.32941177  0.29019609  0.20392157]\n",
      "   ..., \n",
      "   [ 0.3882353   0.36470589  0.27450982]\n",
      "   [ 0.35294119  0.32941177  0.23921569]\n",
      "   [ 0.31764707  0.29411766  0.20392157]]\n",
      "\n",
      "  [[ 0.34509805  0.28235295  0.2       ]\n",
      "   [ 0.35294119  0.29019609  0.20392157]\n",
      "   [ 0.36470589  0.3019608   0.21960784]\n",
      "   ..., \n",
      "   [ 0.3137255   0.29019609  0.20784314]\n",
      "   [ 0.29803923  0.27450982  0.19215687]\n",
      "   [ 0.32156864  0.29803923  0.21568628]]\n",
      "\n",
      "  [[ 0.38039216  0.30588236  0.21960784]\n",
      "   [ 0.36862746  0.29411766  0.20784314]\n",
      "   [ 0.36470589  0.29411766  0.20784314]\n",
      "   ..., \n",
      "   [ 0.21176471  0.18431373  0.10980392]\n",
      "   [ 0.24705882  0.21960784  0.14509805]\n",
      "   [ 0.28235295  0.25490198  0.18039216]]]\n",
      "\n",
      "\n",
      " [[[ 0.66666669  0.70588237  0.7764706 ]\n",
      "   [ 0.65882355  0.69803923  0.76862746]\n",
      "   [ 0.69411767  0.72549021  0.79607844]\n",
      "   ..., \n",
      "   [ 0.63529414  0.7019608   0.84313726]\n",
      "   [ 0.61960787  0.69803923  0.8392157 ]\n",
      "   [ 0.6156863   0.69411767  0.83137256]]\n",
      "\n",
      "  [[ 0.65882355  0.70980394  0.7764706 ]\n",
      "   [ 0.67450982  0.72549021  0.78823531]\n",
      "   [ 0.67058825  0.71764708  0.78431374]\n",
      "   ..., \n",
      "   [ 0.62352943  0.69411767  0.83137256]\n",
      "   [ 0.61176473  0.6901961   0.82745099]\n",
      "   [ 0.60392159  0.68235296  0.81960785]]\n",
      "\n",
      "  [[ 0.60392159  0.66666669  0.72941178]\n",
      "   [ 0.58431375  0.64705884  0.70980394]\n",
      "   [ 0.50588238  0.56470591  0.63529414]\n",
      "   ..., \n",
      "   [ 0.63137257  0.69803923  0.8392157 ]\n",
      "   [ 0.6156863   0.69411767  0.83137256]\n",
      "   [ 0.60392159  0.68235296  0.81960785]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019609  0.32941177  0.3137255 ]\n",
      "   [ 0.29803923  0.33333334  0.31764707]\n",
      "   [ 0.30588236  0.33333334  0.32156864]\n",
      "   ..., \n",
      "   [ 0.27843139  0.29411766  0.30588236]\n",
      "   [ 0.26666668  0.28235295  0.29411766]\n",
      "   [ 0.23921569  0.25490198  0.26666668]]\n",
      "\n",
      "  [[ 0.26666668  0.29803923  0.3019608 ]\n",
      "   [ 0.27058825  0.3019608   0.30588236]\n",
      "   [ 0.28235295  0.30980393  0.30588236]\n",
      "   ..., \n",
      "   [ 0.29803923  0.3137255   0.32549021]\n",
      "   [ 0.27843139  0.29411766  0.30588236]\n",
      "   [ 0.27843139  0.29411766  0.30588236]]\n",
      "\n",
      "  [[ 0.26274511  0.29411766  0.30588236]\n",
      "   [ 0.26666668  0.29803923  0.30980393]\n",
      "   [ 0.27058825  0.29411766  0.29803923]\n",
      "   ..., \n",
      "   [ 0.29411766  0.30980393  0.32156864]\n",
      "   [ 0.27843139  0.29411766  0.30588236]\n",
      "   [ 0.28627452  0.3019608   0.3137255 ]]]]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "#X_test /= 255\n",
    "print(\"train sample:\",X_train[:5])#,\"test sample\",x_test[:5])\n",
    "# from keras.utils import np_utils\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# #save the uncoded labels\n",
    "# labels_data = y_train\n",
    "# # encode class values as integers\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(y_train)\n",
    "# encoded_Y = encoder.transform(y_train)\n",
    "# # print(encoded_Y.shape, encoded_Y) \n",
    "# # [0,1,2,3...,34,35]\n",
    "# # convert integers to dummy variables (i.e. one hot encoded)\n",
    "# dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# # print(dummy_y.shape, dummy_y)\n",
    "# # [1,0,0....0][0,1,0....,0][0,0,1,...,0]\n",
    "\n",
    "# y_train = dummy_y\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.6514 - acc: 0.2916Epoch 00000: loss improved from inf to 2.65075, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 397s - loss: 2.6508 - acc: 0.2918   \n",
      "Epoch 2/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.0389 - acc: 0.3821Epoch 00001: loss improved from 2.65075 to 2.03774, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 386s - loss: 2.0387 - acc: 0.3822   \n",
      "Epoch 3/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.8249 - acc: 0.4197Epoch 00002: loss improved from 2.03774 to 1.82451, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 404s - loss: 1.8250 - acc: 0.4197   \n",
      "Epoch 4/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.6562 - acc: 0.4588Epoch 00003: loss improved from 1.82451 to 1.65641, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 432s - loss: 1.6564 - acc: 0.4587   \n",
      "Epoch 5/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.5348 - acc: 0.4873Epoch 00004: loss improved from 1.65641 to 1.53492, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 401s - loss: 1.5350 - acc: 0.4874   \n",
      "Epoch 6/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.4430 - acc: 0.5132Epoch 00005: loss improved from 1.53492 to 1.44243, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 398s - loss: 1.4428 - acc: 0.5133   \n",
      "Epoch 7/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.3712 - acc: 0.5293Epoch 00006: loss improved from 1.44243 to 1.37098, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 397s - loss: 1.3713 - acc: 0.5292   \n",
      "Epoch 8/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.3235 - acc: 0.5419Epoch 00007: loss improved from 1.37098 to 1.32320, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 384s - loss: 1.3235 - acc: 0.5419   \n",
      "Epoch 9/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.2674 - acc: 0.5598Epoch 00008: loss improved from 1.32320 to 1.26697, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 391s - loss: 1.2672 - acc: 0.5597   \n",
      "Epoch 10/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.2190 - acc: 0.5728Epoch 00009: loss improved from 1.26697 to 1.21882, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 392s - loss: 1.2189 - acc: 0.5728   \n",
      "Epoch 11/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.1845 - acc: 0.5847Epoch 00010: loss improved from 1.21882 to 1.18483, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 389s - loss: 1.1848 - acc: 0.5847   \n",
      "Epoch 12/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.1557 - acc: 0.5950Epoch 00011: loss improved from 1.18483 to 1.15545, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 404s - loss: 1.1557 - acc: 0.5950   \n",
      "Epoch 13/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.1255 - acc: 0.6045Epoch 00012: loss improved from 1.15545 to 1.12571, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 533s - loss: 1.1255 - acc: 0.6045   \n",
      "Epoch 14/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.1010 - acc: 0.6116Epoch 00013: loss improved from 1.12571 to 1.10104, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 622s - loss: 1.1010 - acc: 0.6116   \n",
      "Epoch 15/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.0803 - acc: 0.6198Epoch 00014: loss improved from 1.10104 to 1.07962, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 476s - loss: 1.0802 - acc: 0.6198   \n",
      "Epoch 16/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.0595 - acc: 0.6255Epoch 00015: loss improved from 1.07962 to 1.05989, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 425s - loss: 1.0595 - acc: 0.6255   \n",
      "Epoch 17/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.0388 - acc: 0.6348Epoch 00016: loss improved from 1.05989 to 1.03812, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 395s - loss: 1.0388 - acc: 0.6348   \n",
      "Epoch 18/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.0274 - acc: 0.6387Epoch 00017: loss improved from 1.03812 to 1.02761, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 404s - loss: 1.0277 - acc: 0.6386   \n",
      "Epoch 19/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.0056 - acc: 0.6462Epoch 00018: loss improved from 1.02761 to 1.00521, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 520s - loss: 1.0055 - acc: 0.6462   \n",
      "Epoch 20/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9968 - acc: 0.6520Epoch 00019: loss improved from 1.00521 to 0.99707, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 405s - loss: 0.9968 - acc: 0.6520   \n",
      "Epoch 21/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9847 - acc: 0.6544Epoch 00020: loss improved from 0.99707 to 0.98461, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 398s - loss: 0.9847 - acc: 0.6544   \n",
      "Epoch 22/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9737 - acc: 0.6580Epoch 00021: loss improved from 0.98461 to 0.97349, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 399s - loss: 0.9734 - acc: 0.6581   \n",
      "Epoch 23/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9590 - acc: 0.6627Epoch 00022: loss improved from 0.97349 to 0.95887, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 405s - loss: 0.9590 - acc: 0.6628   \n",
      "Epoch 24/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9497 - acc: 0.6660Epoch 00023: loss improved from 0.95887 to 0.94937, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 398s - loss: 0.9497 - acc: 0.6661   \n",
      "Epoch 25/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9425 - acc: 0.6682Epoch 00024: loss improved from 0.94937 to 0.94234, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 361s - loss: 0.9423 - acc: 0.6682   \n",
      "Epoch 26/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9350 - acc: 0.6713Epoch 00025: loss improved from 0.94234 to 0.93532, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 361s - loss: 0.9350 - acc: 0.6713   \n",
      "Epoch 27/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9190 - acc: 0.6757Epoch 00026: loss improved from 0.93532 to 0.91885, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 357s - loss: 0.9188 - acc: 0.6758   \n",
      "Epoch 28/400\n",
      "780/781 [============================>.] - ETA: 1s - loss: 0.9119 - acc: 0.6784Epoch 00027: loss improved from 0.91885 to 0.91226, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 793s - loss: 0.9120 - acc: 0.6784   \n",
      "Epoch 29/400\n",
      "780/781 [============================>.] - ETA: 1s - loss: 0.9080 - acc: 0.6812Epoch 00028: loss improved from 0.91226 to 0.90783, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 857s - loss: 0.9079 - acc: 0.6812   \n",
      "Epoch 30/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9047 - acc: 0.6813Epoch 00029: loss improved from 0.90783 to 0.90463, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 409s - loss: 0.9045 - acc: 0.6814   \n",
      "Epoch 31/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8909 - acc: 0.6879Epoch 00030: loss improved from 0.90463 to 0.89128, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 365s - loss: 0.8909 - acc: 0.6879   \n",
      "Epoch 32/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8849 - acc: 0.6891Epoch 00031: loss improved from 0.89128 to 0.88492, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 360s - loss: 0.8848 - acc: 0.6891   \n",
      "Epoch 33/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.6917Epoch 00032: loss improved from 0.88492 to 0.87864, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 384s - loss: 0.8785 - acc: 0.6918   \n",
      "Epoch 34/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8707 - acc: 0.6961Epoch 00033: loss improved from 0.87864 to 0.87007, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 365s - loss: 0.8706 - acc: 0.6962   \n",
      "Epoch 35/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8701 - acc: 0.6966Epoch 00034: loss improved from 0.87007 to 0.86954, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 389s - loss: 0.8699 - acc: 0.6966   \n",
      "Epoch 36/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8596 - acc: 0.7001Epoch 00035: loss improved from 0.86954 to 0.85969, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 421s - loss: 0.8595 - acc: 0.7001   \n",
      "Epoch 37/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8485 - acc: 0.7017Epoch 00036: loss improved from 0.85969 to 0.84834, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 433s - loss: 0.8486 - acc: 0.7016   \n",
      "Epoch 38/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8488 - acc: 0.7032Epoch 00037: loss did not improve\n",
      "781/781 [==============================] - 494s - loss: 0.8485 - acc: 0.7033   \n",
      "Epoch 39/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8392 - acc: 0.7049Epoch 00038: loss improved from 0.84834 to 0.83916, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 422s - loss: 0.8392 - acc: 0.7049   \n",
      "Epoch 40/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8378 - acc: 0.7061Epoch 00039: loss improved from 0.83916 to 0.83817, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 401s - loss: 0.8379 - acc: 0.7061   \n",
      "Epoch 41/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8332 - acc: 0.7090Epoch 00040: loss improved from 0.83817 to 0.83319, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 422s - loss: 0.8331 - acc: 0.7091   \n",
      "Epoch 42/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8281 - acc: 0.7103Epoch 00041: loss improved from 0.83319 to 0.82804, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 402s - loss: 0.8283 - acc: 0.7103   \n",
      "Epoch 43/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8257 - acc: 0.7117Epoch 00042: loss improved from 0.82804 to 0.82534, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 394s - loss: 0.8256 - acc: 0.7117   \n",
      "Epoch 44/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8185 - acc: 0.7119Epoch 00043: loss improved from 0.82534 to 0.81832, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 413s - loss: 0.8186 - acc: 0.7118   \n",
      "Epoch 45/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8153 - acc: 0.7148Epoch 00044: loss improved from 0.81832 to 0.81535, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 399s - loss: 0.8152 - acc: 0.7148   \n",
      "Epoch 46/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8121 - acc: 0.7142Epoch 00045: loss improved from 0.81535 to 0.81246, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 382s - loss: 0.8122 - acc: 0.7141   \n",
      "Epoch 47/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8154 - acc: 0.7154Epoch 00046: loss did not improve\n",
      "781/781 [==============================] - 417s - loss: 0.8154 - acc: 0.7154   \n",
      "Epoch 48/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7994 - acc: 0.7200Epoch 00047: loss improved from 0.81246 to 0.79946, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 430s - loss: 0.7994 - acc: 0.7201   \n",
      "Epoch 49/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8022 - acc: 0.7193Epoch 00048: loss did not improve\n",
      "781/781 [==============================] - 403s - loss: 0.8021 - acc: 0.7194   \n",
      "Epoch 50/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7963 - acc: 0.7221Epoch 00049: loss improved from 0.79946 to 0.79590, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 349s - loss: 0.7963 - acc: 0.7221   \n",
      "Epoch 51/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7918 - acc: 0.7227Epoch 00050: loss improved from 0.79590 to 0.79203, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 352s - loss: 0.7919 - acc: 0.7226   \n",
      "Epoch 52/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7894 - acc: 0.7240Epoch 00051: loss improved from 0.79203 to 0.78943, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 346s - loss: 0.7895 - acc: 0.7240   \n",
      "Epoch 53/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7888 - acc: 0.7253Epoch 00052: loss improved from 0.78943 to 0.78842, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 348s - loss: 0.7889 - acc: 0.7253   \n",
      "Epoch 54/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7836 - acc: 0.7258Epoch 00053: loss improved from 0.78842 to 0.78343, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 348s - loss: 0.7835 - acc: 0.7258   \n",
      "Epoch 55/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.7294Epoch 00054: loss improved from 0.78343 to 0.77562, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 354s - loss: 0.7757 - acc: 0.7295   \n",
      "Epoch 56/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7731 - acc: 0.7323Epoch 00055: loss improved from 0.77562 to 0.77337, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 347s - loss: 0.7732 - acc: 0.7323   \n",
      "Epoch 57/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7728 - acc: 0.7294Epoch 00056: loss improved from 0.77337 to 0.77243, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 346s - loss: 0.7725 - acc: 0.7295   \n",
      "Epoch 58/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7709 - acc: 0.7308Epoch 00057: loss improved from 0.77243 to 0.77110, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 349s - loss: 0.7711 - acc: 0.7308   \n",
      "Epoch 59/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7716 - acc: 0.7300Epoch 00058: loss improved from 0.77110 to 0.77048, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 351s - loss: 0.7715 - acc: 0.7300   \n",
      "Epoch 60/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7614 - acc: 0.7348Epoch 00059: loss improved from 0.77048 to 0.76174, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 363s - loss: 0.7613 - acc: 0.7348   \n",
      "Epoch 61/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7589 - acc: 0.7365Epoch 00060: loss improved from 0.76174 to 0.75886, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 371s - loss: 0.7590 - acc: 0.7364   \n",
      "Epoch 62/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7552 - acc: 0.7355Epoch 00061: loss improved from 0.75886 to 0.75520, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 349s - loss: 0.7551 - acc: 0.7355   \n",
      "Epoch 63/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/781 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.7387Epoch 00062: loss improved from 0.75520 to 0.75289, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 351s - loss: 0.7531 - acc: 0.7388   \n",
      "Epoch 64/400\n",
      "780/781 [============================>.] - ETA: 1s - loss: 0.7512 - acc: 0.7363Epoch 00063: loss improved from 0.75289 to 0.75093, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 886s - loss: 0.7513 - acc: 0.7363   \n",
      "Epoch 65/400\n",
      "780/781 [============================>.] - ETA: 1s - loss: 0.7500 - acc: 0.7392Epoch 00064: loss improved from 0.75093 to 0.74961, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 1103s - loss: 0.7500 - acc: 0.7393  \n",
      "Epoch 66/400\n",
      "780/781 [============================>.] - ETA: 1s - loss: 0.7483 - acc: 0.7395Epoch 00065: loss improved from 0.74961 to 0.74817, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 1045s - loss: 0.7483 - acc: 0.7395  \n",
      "Epoch 67/400\n",
      "780/781 [============================>.] - ETA: 1s - loss: 0.7447 - acc: 0.7390Epoch 00066: loss improved from 0.74817 to 0.74481, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 1101s - loss: 0.7446 - acc: 0.7391  \n",
      "Epoch 68/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7447 - acc: 0.7402Epoch 00067: loss improved from 0.74481 to 0.74422, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 621s - loss: 0.7444 - acc: 0.7403   \n",
      "Epoch 69/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7372 - acc: 0.7445Epoch 00068: loss improved from 0.74422 to 0.73723, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 349s - loss: 0.7372 - acc: 0.7444   \n",
      "Epoch 70/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7382 - acc: 0.7427Epoch 00069: loss did not improve\n",
      "781/781 [==============================] - 355s - loss: 0.7385 - acc: 0.7427   \n",
      "Epoch 71/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7334 - acc: 0.7454Epoch 00070: loss improved from 0.73723 to 0.73353, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 360s - loss: 0.7332 - acc: 0.7454   \n",
      "Epoch 72/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7319 - acc: 0.7441Epoch 00071: loss improved from 0.73353 to 0.73196, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 356s - loss: 0.7320 - acc: 0.7440   \n",
      "Epoch 73/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.7455Epoch 00072: loss improved from 0.73196 to 0.73030, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 359s - loss: 0.7304 - acc: 0.7455   \n",
      "Epoch 74/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7219 - acc: 0.7480Epoch 00073: loss improved from 0.73030 to 0.72246, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 364s - loss: 0.7222 - acc: 0.7479   \n",
      "Epoch 75/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7284 - acc: 0.7458Epoch 00074: loss did not improve\n",
      "781/781 [==============================] - 390s - loss: 0.7285 - acc: 0.7457   \n",
      "Epoch 76/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7228 - acc: 0.7476Epoch 00075: loss improved from 0.72246 to 0.72203, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 464s - loss: 0.7227 - acc: 0.7477   \n",
      "Epoch 77/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7275 - acc: 0.7468Epoch 00076: loss did not improve\n",
      "781/781 [==============================] - 423s - loss: 0.7280 - acc: 0.7467   \n",
      "Epoch 78/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7157 - acc: 0.7507Epoch 00077: loss improved from 0.72203 to 0.71532, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 458s - loss: 0.7157 - acc: 0.7507   \n",
      "Epoch 79/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7148 - acc: 0.7509Epoch 00078: loss improved from 0.71532 to 0.71457, saving model to backups/weights.best.hdf5\n",
      "781/781 [==============================] - 492s - loss: 0.7147 - acc: 0.7509   \n",
      "Epoch 80/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7171 - acc: 0.7501Epoch 00079: loss did not improve\n",
      "781/781 [==============================] - 438s - loss: 0.7170 - acc: 0.7501   \n",
      "Epoch 81/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7146 - acc: 0.7497Epoch 00080: loss did not improve\n",
      "781/781 [==============================] - 447s - loss: 0.7144 - acc: 0.7498   \n",
      "Epoch 82/400\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7158 - acc: 0.7500Epoch 00081: loss did not improve\n",
      "781/781 [==============================] - 397s - loss: 0.7159 - acc: 0.7500   \n",
      "Epoch 00081: early stopping\n",
      "debuging\n"
     ]
    }
   ],
   "source": [
    "model = my_deeper_cnn_model()\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_split=0.01,\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(X_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        epochs=epochs, callbacks=callbacks)#,validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Fit keras Model\n",
    "# model.fit(X_train, Y_train,\n",
    "#           batch_size=256, epochs=30, verbose=1, callbacks=callbacks,validation_data=(X_test,Y_test)) #instead of 15epoch\n",
    "#model.fit(images_data, labels_data,\n",
    "#          batch_size=256, epochs=18, verbose=1, validation_split=0.0001) #instead of 15epoch\n",
    "# callbacks=callbacks,\n",
    "# Epoch 1/10\n",
    "# 7744/60000 [==>...........................] - ETA: 96s - loss: 0.5806 - acc: 0.8164\n",
    "\n",
    "#score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "print('debuging')\n",
    "#\n",
    "# from keras.models import model_from_json\n",
    "#\n",
    "# # split into input (X) and output (Y) variables\n",
    "# # X = dataset[:,0:8]\n",
    "# # Y = dataset[:,8]\n",
    "# # # create model\n",
    "# # model = Sequential()\n",
    "# # model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "# # model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "# # model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# # # Compile model\n",
    "# # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # # Fit the model\n",
    "# # model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
    "# # # evaluate the model\n",
    "# # scores = model.evaluate(X, Y, verbose=0)\n",
    "# # print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#\n",
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"model.h5\")\n",
    "# print(\"Saved model to disk\")\n",
    "#\n",
    "# # later...\n",
    "#\n",
    "# # load json and create model\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "#\n",
    "# # evaluate loaded model on test data\n",
    "# loaded_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n",
    "# from keras.models import load_model\n",
    "#\n",
    "# model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# #del model  # deletes the existing model\n",
    "#\n",
    "# # returns a compiled model\n",
    "# # identical to the previous one\n",
    "# model_L = load_model('my_model.h5')\n",
    "# model_L.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# score = model_L.evaluate(X_test, Y_test, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (model_L.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sample: [[[[ 0.24313726  0.23137255  0.27843139]\n",
      "   [ 0.47450981  0.47843137  0.45882353]\n",
      "   [ 0.49803922  0.52156866  0.43921569]\n",
      "   ..., \n",
      "   [ 0.32941177  0.30588236  0.31764707]\n",
      "   [ 0.32941177  0.29803923  0.31764707]\n",
      "   [ 0.30980393  0.27450982  0.30588236]]\n",
      "\n",
      "  [[ 0.25490198  0.23529412  0.28235295]\n",
      "   [ 0.50588238  0.51764709  0.47843137]\n",
      "   [ 0.48627451  0.52156866  0.41568628]\n",
      "   ..., \n",
      "   [ 0.5411765   0.50980395  0.52941179]\n",
      "   [ 0.52549022  0.48627451  0.50980395]\n",
      "   [ 0.49411765  0.45490196  0.48627451]]\n",
      "\n",
      "  [[ 0.24705882  0.23137255  0.27843139]\n",
      "   [ 0.53725493  0.5529412   0.49803922]\n",
      "   [ 0.48235294  0.52156866  0.40784314]\n",
      "   ..., \n",
      "   [ 0.41568628  0.3764706   0.40784314]\n",
      "   [ 0.42745098  0.38431373  0.41568628]\n",
      "   [ 0.43137255  0.3882353   0.41960785]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.6156863   0.66666669  0.52941179]\n",
      "   [ 0.6901961   0.74901962  0.60000002]\n",
      "   [ 0.78039217  0.83529413  0.67450982]\n",
      "   ..., \n",
      "   [ 0.24705882  0.26274511  0.28627452]\n",
      "   [ 0.30980393  0.32156864  0.35294119]\n",
      "   [ 0.16470589  0.17647059  0.20784314]]\n",
      "\n",
      "  [[ 0.52549022  0.55686277  0.48627451]\n",
      "   [ 0.63137257  0.66666669  0.58039218]\n",
      "   [ 0.73333335  0.77254903  0.65882355]\n",
      "   ..., \n",
      "   [ 0.25098041  0.26666668  0.29019609]\n",
      "   [ 0.29411766  0.30588236  0.33725491]\n",
      "   [ 0.16078432  0.17254902  0.20784314]]\n",
      "\n",
      "  [[ 0.26274511  0.25098041  0.29019609]\n",
      "   [ 0.30980393  0.29019609  0.32549021]\n",
      "   [ 0.32156864  0.3019608   0.33725491]\n",
      "   ..., \n",
      "   [ 0.24705882  0.26274511  0.29019609]\n",
      "   [ 0.27058825  0.28627452  0.32156864]\n",
      "   [ 0.14901961  0.16470589  0.19607843]]]\n",
      "\n",
      "\n",
      " [[[ 0.72156864  0.75686276  0.75686276]\n",
      "   [ 0.58431375  0.627451    0.627451  ]\n",
      "   [ 0.53333336  0.58039218  0.58431375]\n",
      "   ..., \n",
      "   [ 0.54901963  0.51372552  0.48627451]\n",
      "   [ 0.47843137  0.44313726  0.41176471]\n",
      "   [ 0.49411765  0.46666667  0.41176471]]\n",
      "\n",
      "  [[ 0.40784314  0.47058824  0.49019608]\n",
      "   [ 0.35686275  0.43529412  0.45882353]\n",
      "   [ 0.37254903  0.44313726  0.47450981]\n",
      "   ..., \n",
      "   [ 0.54901963  0.53333336  0.49803922]\n",
      "   [ 0.50196081  0.45490196  0.43529412]\n",
      "   [ 0.53725493  0.50196081  0.4627451 ]]\n",
      "\n",
      "  [[ 0.44313726  0.48235294  0.51764709]\n",
      "   [ 0.44705883  0.49803922  0.53333336]\n",
      "   [ 0.47450981  0.51764709  0.54509807]\n",
      "   ..., \n",
      "   [ 0.58823532  0.60392159  0.49803922]\n",
      "   [ 0.67450982  0.60392159  0.52941179]\n",
      "   [ 0.64313728  0.56862748  0.48235294]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.75294119  0.60784316  0.50588238]\n",
      "   [ 0.7019608   0.59215689  0.51372552]\n",
      "   [ 0.50196081  0.46666667  0.41568628]\n",
      "   ..., \n",
      "   [ 0.47058824  0.44705883  0.40784314]\n",
      "   [ 0.4627451   0.40392157  0.34509805]\n",
      "   [ 0.50588238  0.41176471  0.33333334]]\n",
      "\n",
      "  [[ 0.66666669  0.50588238  0.40000001]\n",
      "   [ 0.66274512  0.52549022  0.45490196]\n",
      "   [ 0.47843137  0.42352942  0.3764706 ]\n",
      "   ..., \n",
      "   [ 0.42352942  0.41568628  0.36470589]\n",
      "   [ 0.49803922  0.44705883  0.3764706 ]\n",
      "   [ 0.54509807  0.45882353  0.36862746]]\n",
      "\n",
      "  [[ 0.63921571  0.48235294  0.38039216]\n",
      "   [ 0.56862748  0.45882353  0.38431373]\n",
      "   [ 0.40000001  0.39215687  0.34901962]\n",
      "   ..., \n",
      "   [ 0.44313726  0.43137255  0.35686275]\n",
      "   [ 0.55686277  0.50196081  0.41176471]\n",
      "   [ 0.627451    0.52941179  0.42745098]]]\n",
      "\n",
      "\n",
      " [[[ 0.57647061  0.53333336  0.42745098]\n",
      "   [ 0.78823531  0.76078433  0.65882355]\n",
      "   [ 0.93333334  0.91764706  0.82352942]\n",
      "   ..., \n",
      "   [ 0.47843137  0.43137255  0.3019608 ]\n",
      "   [ 0.52549022  0.48627451  0.35294119]\n",
      "   [ 0.49019608  0.4627451   0.33725491]]\n",
      "\n",
      "  [[ 0.90980393  0.88627452  0.8509804 ]\n",
      "   [ 0.97647059  0.97254902  0.94901961]\n",
      "   [ 0.96862745  0.95686275  0.94509804]\n",
      "   ..., \n",
      "   [ 0.52549022  0.48235294  0.39215687]\n",
      "   [ 0.52156866  0.49019608  0.3764706 ]\n",
      "   [ 0.51372552  0.49019608  0.36862746]]\n",
      "\n",
      "  [[ 0.97647059  0.97647059  0.96470588]\n",
      "   [ 0.98039216  0.97254902  0.98431373]\n",
      "   [ 0.97647059  0.96862745  0.98431373]\n",
      "   ..., \n",
      "   [ 0.6156863   0.57254905  0.49803922]\n",
      "   [ 0.51764709  0.49411765  0.38039216]\n",
      "   [ 0.5529412   0.54509807  0.39607844]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.30588236  0.31764707  0.18039216]\n",
      "   [ 0.27843139  0.29019609  0.15294118]\n",
      "   [ 0.24705882  0.26666668  0.1254902 ]\n",
      "   ..., \n",
      "   [ 0.38039216  0.42745098  0.25098041]\n",
      "   [ 0.33725491  0.3882353   0.21568628]\n",
      "   [ 0.36862746  0.41960785  0.25882354]]\n",
      "\n",
      "  [[ 0.32549021  0.3137255   0.19215687]\n",
      "   [ 0.32549021  0.32156864  0.19607843]\n",
      "   [ 0.33725491  0.33725491  0.21176471]\n",
      "   ..., \n",
      "   [ 0.36862746  0.40000001  0.24705882]\n",
      "   [ 0.3764706   0.41568628  0.25882354]\n",
      "   [ 0.36078432  0.40392157  0.24705882]]\n",
      "\n",
      "  [[ 0.3137255   0.29411766  0.19607843]\n",
      "   [ 0.29411766  0.27450982  0.18039216]\n",
      "   [ 0.30588236  0.29411766  0.19215687]\n",
      "   ..., \n",
      "   [ 0.3137255   0.33725491  0.19607843]\n",
      "   [ 0.34509805  0.38039216  0.22352941]\n",
      "   [ 0.3764706   0.41568628  0.25098041]]]\n",
      "\n",
      "\n",
      " [[[ 0.41960785  0.51764709  0.7019608 ]\n",
      "   [ 0.39215687  0.47843137  0.61176473]\n",
      "   [ 0.36470589  0.43921569  0.52941179]\n",
      "   ..., \n",
      "   [ 0.24313726  0.2         0.20784314]\n",
      "   [ 0.20392157  0.15294118  0.18039216]\n",
      "   [ 0.21176471  0.14509805  0.19607843]]\n",
      "\n",
      "  [[ 0.39607844  0.48235294  0.63529414]\n",
      "   [ 0.3764706   0.44705883  0.53725493]\n",
      "   [ 0.35686275  0.42352942  0.46666667]\n",
      "   ..., \n",
      "   [ 0.29019609  0.24705882  0.23921569]\n",
      "   [ 0.27058825  0.21960784  0.21960784]\n",
      "   [ 0.24313726  0.18039216  0.20392157]]\n",
      "\n",
      "  [[ 0.37254903  0.44313726  0.54509807]\n",
      "   [ 0.36078432  0.41568628  0.47450981]\n",
      "   [ 0.34117648  0.3882353   0.41176471]\n",
      "   ..., \n",
      "   [ 0.29803923  0.29411766  0.31764707]\n",
      "   [ 0.24705882  0.23137255  0.26274511]\n",
      "   [ 0.22352941  0.2         0.23529412]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.62352943  0.70980394  0.96470588]\n",
      "   [ 0.65882355  0.7647059   0.96470588]\n",
      "   [ 0.71372551  0.81176472  0.96862745]\n",
      "   ..., \n",
      "   [ 0.52156866  0.60784316  0.60784316]\n",
      "   [ 0.50588238  0.58823532  0.62352943]\n",
      "   [ 0.48627451  0.58039218  0.62352943]]\n",
      "\n",
      "  [[ 0.60392159  0.69411767  0.96862745]\n",
      "   [ 0.64705884  0.76078433  0.97254902]\n",
      "   [ 0.69803923  0.80784315  0.97254902]\n",
      "   ..., \n",
      "   [ 0.53725493  0.62352943  0.67450982]\n",
      "   [ 0.53333336  0.61176473  0.68627453]\n",
      "   [ 0.51764709  0.60000002  0.67450982]]\n",
      "\n",
      "  [[ 0.60000002  0.69803923  0.97647059]\n",
      "   [ 0.63529414  0.75686276  0.98039216]\n",
      "   [ 0.68627453  0.80000001  0.98039216]\n",
      "   ..., \n",
      "   [ 0.56862748  0.66666669  0.73725492]\n",
      "   [ 0.56862748  0.65098041  0.74117649]\n",
      "   [ 0.56078434  0.63529414  0.72156864]]]\n",
      "\n",
      "\n",
      " [[[ 0.18039216  0.1254902   0.18039216]\n",
      "   [ 0.67058825  0.70588237  0.87058824]\n",
      "   [ 0.68627453  0.76862746  0.93333334]\n",
      "   ..., \n",
      "   [ 0.20784314  0.17647059  0.12156863]\n",
      "   [ 0.15294118  0.08235294  0.16470589]\n",
      "   [ 0.12156863  0.16078432  0.16078432]]\n",
      "\n",
      "  [[ 0.20392157  0.28627452  0.25882354]\n",
      "   [ 0.82352942  0.70980394  0.89803922]\n",
      "   [ 0.66666669  0.7764706   0.90980393]\n",
      "   ..., \n",
      "   [ 0.35294119  0.35294119  0.41568628]\n",
      "   [ 0.34117648  0.33333334  0.40784314]\n",
      "   [ 0.36862746  0.36470589  0.29019609]]\n",
      "\n",
      "  [[ 0.32941177  0.27450982  0.25490198]\n",
      "   [ 0.74509805  0.79607844  0.90196079]\n",
      "   [ 0.71764708  0.75686276  0.88627452]\n",
      "   ..., \n",
      "   [ 0.23921569  0.25490198  0.18431373]\n",
      "   [ 0.32549021  0.38039216  0.39607844]\n",
      "   [ 0.28627452  0.32941177  0.23529412]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.30980393  0.09411765  0.10980392]\n",
      "   [ 0.3019608   0.21176471  0.23137255]\n",
      "   [ 0.23529412  0.21568628  0.22745098]\n",
      "   ..., \n",
      "   [ 0.01568628  0.07058824  0.05882353]\n",
      "   [ 0.          0.00392157  0.08627451]\n",
      "   [ 0.          0.02352941  0.05490196]]\n",
      "\n",
      "  [[ 0.28235295  0.25882354  0.23921569]\n",
      "   [ 0.32156864  0.23529412  0.1254902 ]\n",
      "   [ 0.34117648  0.23529412  0.20784314]\n",
      "   ..., \n",
      "   [ 0.18431373  0.18039216  0.12156863]\n",
      "   [ 0.22352941  0.11372549  0.14901961]\n",
      "   [ 0.10196079  0.00392157  0.03921569]]\n",
      "\n",
      "  [[ 0.0627451   0.          0.05490196]\n",
      "   [ 0.16470589  0.12941177  0.05882353]\n",
      "   [ 0.11764706  0.08235294  0.11764706]\n",
      "   ..., \n",
      "   [ 0.46666667  0.38039216  0.27058825]\n",
      "   [ 0.40000001  0.3137255   0.28235295]\n",
      "   [ 0.19215687  0.08235294  0.04705882]]]]\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "print(\"test sample:\",X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 46s    \n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# rst = model_L.predict_classes(images_datats, verbose=0)\n",
    "# print(rst.shape)\n",
    "\n",
    "rst = model.predict_classes(X_test, verbose=1)\n",
    "print(rst.shape)\n",
    "rststr = encoder.inverse_transform(rst)\n",
    "rst_df = pd.DataFrame({\"id\": list(range(230001,230000+len(rst)+1)),\n",
    "                         \"label\": rststr})\n",
    "rst_df.to_csv('rstpreditct0806_230_250.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(rststr))\n",
    "print(len(list(range(50001,50000+len(rst)+1))))\n",
    "print(list(range(50001,50001+len(rst)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# del model  # deletes the existing model\n",
    "\n",
    "model_L = load_model('my_model.h5')\n",
    "model_L.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "score = model_L.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_L.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use pretrained model\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "import h5py\n",
    "def write_gap(MODEL, image_size, lambda_func=None):\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    gen = ImageDataGenerator()\n",
    "    train_generator = gen.flow_from_directory(\"train2\", image_size, shuffle=False, \n",
    "                                              batch_size=16)\n",
    "    test_generator = gen.flow_from_directory(\"test2\", image_size, shuffle=False, \n",
    "                                             batch_size=16, class_mode=None)\n",
    "    train = model.predict_generator(train_generator, train_generator.nb_sample)\n",
    "    test = model.predict_generator(test_generator, test_generator.nb_sample)\n",
    "    with h5py.File(\"gap_%s.h5\"%MODEL.func_name) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)\n",
    "write_gap(ResNet50, (224, 224))\n",
    "write_gap(InceptionV3, (299, 299), inception_v3.preprocess_input)\n",
    "write_gap(Xception, (299, 299), xception.preprocess_input)\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(2017)\n",
    "X_train = []\n",
    "X_test = []\n",
    "for filename in [\"gap_ResNet50.h5\", \"gap_Xception.h5\", \"gap_InceptionV3.h5\"]:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_test.append(np.array(h['test']))\n",
    "        y_train = np.array(h['label'])\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "np.random.seed(2017)\n",
    "input_tensor = Input(X_train.shape[1:])\n",
    "x = Dropout(0.5)(input_tensor)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(input_tensor, x)\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "digraph G{\n",
    "    node [shape=record]\n",
    "    a[label=\"ResNet50|{input:|output:}|{(224, 224, 3)|(2048)}\"]\n",
    "    b[label=\"InceptionV3|{input:|output:}|{(299, 299, 3)|(2048)}\"]\n",
    "    c[label=\"Xception|{input:|output:}|{(299, 299, 3)|(2048)}\"]\n",
    "    Merge[label=\"Merge|{input:|output:}|{(3, 2048)|(6144)}\"]\n",
    "    Dropout[label=\"Dropout|Rate:|0.5\"]\n",
    "    Output[label=\"Output|{input:|output:}|{(6144)|(1)}\"]\n",
    "    Image -> a -> Merge\n",
    "    Image -> b -> Merge\n",
    "    Image -> c -> Merge\n",
    "    Merge -> Dropout -> Output\n",
    "}\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, nb_epoch=8, validation_split=0.2)\n",
    "\n",
    "rst = model.predict_classes(X_test, verbose=1)\n",
    "print(rst.shape)\n",
    "rststr = encoder.inverse_transform(rst)\n",
    "rst_df = pd.DataFrame({\"id\": list(range(1,len(rst)+1)),\n",
    "                         \"label\": rststr})\n",
    "rst_df.to_csv('rstpreditct0723ooo.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
